https://www.golinuxcloud.com/linux-commands-cheat-sheet/   ---> 100 commands of linux
sudo lsof -i -P -->need to check port

sudo service grafana-server start

ps ax

ps -u user[name or id] -->display processes of the specified effective user name or ID.


Before patching we will need to do downtime in checkMk
take the snapshot of the confluecne 
while going to perform in console using screen
***********************************************************
sudo su -
ps axfn
grep -ir max /etc/nginx/
free
ps -Al
ps ef | grep checkmk
find . -name checkmk.*
netstat - plutn

sudo su -
tail -f /var/log/amazon/ssm/amazon-ssm-agent.log
ps axfn

netstat -plutn
telenet localhost 22
yum install telnet
ssh ip adrees
route -n | findstr 10.12
route print | findstr 10.12
grep -r 3000 /etc/grafana
grep -r 80 /etc/grafana
***********************************************
Visual studio team foundation server 2015
TFS 2015 folder --> right click and open --> tfs server --> you can run as adminstrator


curl http://169.254.169.254/latest/   --> you can find EC2 instance metadata


*********************************************************************
#!/usr/bin/env bash
URL=https://www.atlassian.com/software/confluence/downloads/binary/
wget $URL$(wget -O- $URL |egrep -o 'atlassian-confluence-[0-9]\.[0-9][0-9]\.[0-9\.]+-x64.bin' | sort -V | tail -1)
https://www.atlassian.com/software/confluence/downloads/binary/atlassian-confluence-7.17.1-x64.bin
___________________________________________________________________________________________________________________________________
kubernetes
_____________________________________________________________________________________________________________________________________
Docker installation:
cat /etc/*release*  --> check  Bionic server
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
reference: https://docs.docker.com/engine/install/ubuntu/

docker ps -a
docker run sanju/webapp
docker run -d sanju/webapp
docker attach [containerID]

docker exec [nameofImage] cat /etc/hosts

Pre-Requisite:
AWS account
Installing Kubectl CLI
EKS Cluster ROle
IAM Role For Node Group
VPC
Ec2 Keypair
Aws basics

Kubernetes:
pod.yml
Kubectl apply
Kubectl apply -f pod.yml
kubectl get pods
kubectl describe pod ngnix
kubectl run redis --image=redis123 --dry-run=client -o yaml >redis-definition.yaml

kubectl get pods
kubectl run nginx --image=nginx
kubectl get pods
kubectl get pods
kubectl describe newpods-9xp7q | grep -i image
kubectl describe pod newpods-9xp7q | grep -i image
kubectl get pods -o wide
kubectl get pods

kubectl run redis --image=redis123 --dry-run=client -o yaml > pod.yaml
kubectl get nodes
kubectl descibe node [nameofnode]
kubectl descibe node [nameofnode] | grep Operating

Replica controller

Replica set having selector defination
It's identify the one prod for 100

selector:
  matchLabels:
	tier:front-end
	
kubectl create -f replicaset-definition.yml
kubectl  get replicaset
kubectl delete replicaset myapp-replicaset
kubectl edit replicaset myapp-replicaset

Scale:
kubectl replace -f replicaset-definition.yml
kubectl scale --relicas=6 -f replicaset-definition.yml
kubectl scale --relicas=6 replicaset myapp-replicaset

kubectl replace --force -f file name

Deployments:

kubectl create -f deployment-definition.yaml

kubectl create -f deployment-definition.yaml --record   ---> it's tracked in revision in history

kubectl apply -f deployment-definition.yaml
kubectl get deployments
kubectl get replicacontrollers

kubectl get replicaset
kubectl get pods
kubectl get all
kubectl scale deployment --replicas=3 httpd-frontend

Rollout:
kubectl rollout status deployment/myapp-deployment
kubectl rollout history deployment/myapp-deployment

kubectl apply -f deployment-defination.yml
kubectl set image deployment/myapp-deployment \
			nginx=ngnix:1.9.1
kubectl rollout undo deployment/myapp-deployment


Networking :
IP address is assigned to a POD


Service:
Service-definition.yaml

kubectl creat -f service-definationy.yml
svc= service
kubectl get services
curl http://192.168.1.2.:300008

kubectl describe service {servicename}	---> check target port, need to configured labels, endpoint,

From command also need to edit yaml file
kubectl expose deployment simple-webapp-deployment --name=webapp-service --target-port=8080 --type=NodePort --port=8080 --dry-run=client -o yaml > service.yaml
kubectl apply -f svc.yml


Microservices:

Links: two containers togather

docker run --name=redis redis
docker run -d --name=db postgres:9.4
docker run -d --name=vote -p 5000:5000 --link redis:redis voting-app
docker run -d --name=result -p 5001:80 result-app --link db:db
docker run -d --name=worker worker --link db:db --link redis:redis


Deploy containers
enable connectivity
external access

Deploy pods
crerate services(ClusterIP)
  redis
  db
create services(NodePort)
 voting-app
 result-app
 
 
 
 minikube service voting-service --url   ---> get external IP with port
 
 kubectl create -f redis-service.yaml
 kubectl create -f redis-pod.yaml
 kubectl get pods, svc
 
  
  
  
 kubectl config set-context $(kubectl config current-context) --namespace=dev
 kubectl get pods --namespace=research
 kubectl get pods --all-namespaces | grep blue
 
kubectl get pods
kubectl get pods --namespace=dev
kubectl get pods --namespace=bu
kubectl get pods --selector env=dev
kubectl get pods --selector env=bu
kubectl get pods --selector bu=finance
kubectl get all --selector env=prod --no-headers | wc -l
kubectl get all --selector env=prod,bu=finance,tier=frontend

Node Selector: Select the size of node like small or medium, large


kubectl get nodes
kubedam upgrade plan 
cat /etc/*releases*
apt update
apt install kubeadm=1.20.0-00
kubeadm upgrade apply v1.20.0
apt install kubelet=1.20.0-00
systemctl restart kubelet

upgrading workernode:
ssh node01 or ssh [internalIP]
apt update
apt install kubeadm=1.20.0-00
kubeadm upgrade node
apt install kubelet=1.20.0-00
systemctl daemon-reload
systemctl restart kubelet



k get pods --all-namespaces
k describe pod etcd-controlplane -n kube-system
export ETCDCTL_API=3
etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot save /opt/snapshot-pre-boot.db
etcdctl --data-dir /var/lib/etcd-from-backup snapshot restore /opt/snapshot-pre-boot.db
ls /var/lib/etcd-from-backup/
vi /etc/kubernetes/manifests/etcd.yaml 
k get pods --all-namespace
k get pods
___________________________________________________________________________________________________________________________________

df -kh
mkdir -p /mnt/temp/
mount /dev/nvme1n1p1 /mnt/temp/
ls -al
dmesg -c

we have attach the volume 
we have to start acc instance
after that we will attach the old volume to acc confluence instance
we will work acc instance
dmesg -c
create the directory 
mkdir -p /mnt/temp/
fdisk -l /dev/nvme1n1p1
fdisk -l
mount /dev/nvme1n1p1 /mnt/temp/
dmesg
logout

detatch volume from acc confluence instance 
 
 need to create volume, AZ in 1b region
and attach the volume to acc-jira mission

login to acc-jira console
dmesg
fdisk -l
mkdir -p /mnt/temp/
mount /dev/nvme1n1p1 /mnt/temp/


we need to creat the new instance
VPN:prd-mb-internal-services
Subnet:prd-mb-internal-services-private-1b
IAM Role:iam-ECSEC2profile-14
Next storage
next tags
security groups:prd-sg-confluenceSG
nextkey pair: atlassian |RSA
and lanch instances


detach the volume  from jira 
attach the volume to new confluence instance
in prd-confluence

fdisk -l
mkdir -p /mnt/temp/
mount /dev/nvme1n1p1 /mnt/temp/
blkid
lsblk
fdisk -l
mount /dev/xvdf1 /mnt/temp/
ls -Al /mnt/temp/opt/atlassian/confluence

cat /root/.ssh/authorized_keys

create the new ssh key in new instance
ssh-keygen
cd /root/.ssh/authorized_keys

cat id_rsa.pub
ssh {prd-confluence ip address}
add new secuirty groups to new instacne --> securitygroups-mirabeauVPNSG
create new SG
ssh 10.129.109.93 -v -C -p 22
tail -f /var/log/messages
tcpdump -n -i eth0 host ip adress
ls -AL  /mnt/temp/opt/atlassian/confluence
mkdir -p /mnt/temp/opt/atlassian/confluence
rsync -avzr /mnt/temp/opt/atlassian/confluence ipaddress:/mnt/temp/opt/atlassian/confluenc
rsync -delete /mnt/temp/opt/atlassian/confluence ipaddress:/mnt/temp/opt/atlassian/confluence
ls -lAlht


8********************************
top
screen
screen -dr
cntrl+a d
screen -ls
screen -dr (name of the screen)
netstat -plntu | grep java
ps -axfn | grep java
systemctl status 25822

dig demo.google.com


 
Discard the local changes
using git reset --hard
or git checkout -t -f remote/branch

Or: Discard local changes for a specific file
using git checkout filename


************************************************
if you want udpate the date
sudo apt-get ntpdate ; sudo ntpdate ntp.ubuntu.com

ssh-keygen -f mykey

********************
Modules;
vars.tf
cluster.tf
output.tf
************************
Programming languages
scripting languages
Markup language --> HTML, JSON, Yml
*************************************************************
providers: interact with cloud providers, Saas Providers and other APIs

resources: Each resource block describes one or more infrastructure objects, 
such as virtual networks, compute instances, or higher-level compoments such as DNS records
ex: resource "aws_VPC" --> type of resource "mainvpc" -->define of name
resource "aws_vpc" "main" {
	name = "sanju"
	region = "us-east-1" 
}


variables: Make our deployment more dynamic; A separate file with name "variables.tf" needs to be created in the working 
directory to store all variables for our used in main.tf configuration file

statefile: it's creates a state file to keep track of current state of the infrastructure, it's compares "current state" with desired state using this file
 terraform.tfstate
 
restoring Statefile: if the statefile is deleted or corrupted , we can restore it using terraform import command
 
provisionars: it's ablity to run additional steps or tasks when a resource is created or destoryed; this is not replacement configuration management tool


backends:

modules
data sources
service principals



terraform plan --var-file test.tfvars
terraform apply --var-file test.tfvars
terraform apply --var-file test.tfvars --auto-approve
terraform plan --var-file test.tfvars -var 'aws_access_key=key' -var 'aws-secret_key=secretkey'

Passing creds through terraform -vars option:
terraform plan -var="AWS_ACCESS_KEY_ID=$TF_VAR_AWS_ACCESS_KEY_ID" -var="AWS_ACCESS_KEY_ID=$TF_VAR_AWS_SECRET_ACCESS_KEY"

but I get the same invalid credentials error

Export system manager parameter store credentials within buildspec.yml:
commands:
  - export AWS_ACCESS_KEY_ID=$TF_VAR_AWS_ACCESS_KEY_ID
  - export AWS_SECRET_ACCESS_KEY=$TF_VAR_AWS_SECRET_ACCESS_KEY

which results in duplicate masked variables and the same error above. printenv output within buildspec.yml:

AWS_ACCESS_KEY_ID=***
TF_VAR_AWS_ACCESS_KEY_ID=***
AWS_SECRET_ACCESS_KEY=***
TF_VAR_AWS_SECRET_ACCESS_KEY=***

https://stackoverflow.com/questions/63786188/invalid-terraform-aws-provider-credentials-when-passing-aws-system-manager-param


FOr IAM users --> https://asecure.cloud/a/detect-iam-user-changes/

Genearating CSR file from server

[req]
distinguished_name = req_distinguished_name
req_extensions = v3_req
prompt = no
[req_distinguished_name]
C = NL
ST = Zuid-Holland
L = Rotterdam
O = KPN B.V.
OU = Tooling and Automation
CN = git.kpn.org
[v3_req]
keyUsage = keyEncipherment, dataEncipherment
extendedKeyUsage = serverAuth
subjectAltName = @alt_names
[alt_names]
DNS = git.kpn.org
DNS.1 = git.services.aws.kpn.org


You can add file as san.cnf
openssl genpkey -algorithm RSA -pkeyopt rsa_keygen_bits:2048 -out git.key.pem
openssl req -new -sha256 -key git.key.pem -config san.cnf -out git2022.csr

need to check CSR certificate --> CSR decoder online


opensssl x509 -in /var/lib/kubelet/worker-1.crt -txt





